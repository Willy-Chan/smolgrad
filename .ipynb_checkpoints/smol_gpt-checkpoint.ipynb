{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76141cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-18 23:32:46--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.61MB/s    in 0.4s    \n",
      "\n",
      "2024-12-18 23:32:46 (2.61 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00bd487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0e5a9",
   "metadata": {},
   "source": [
    "# Step 1: Tokenize your text\n",
    "\n",
    "Tokenize essentially means convert every single \"token\"/piece of a word into a __single number__. Here, for simplicity, we will be tokenizing each __character__.\n",
    "\n",
    "GPT-2 uses the byte-pair encoding algorithm. Here we're just going to do a standard character mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e84517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of unique chars:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique chars:  65\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "print(\"List of unique chars: \", unique_chars)\n",
    "print(\"Number of unique chars: \", len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24408e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 46, 47, 57, 1, 47, 57, 1, 39, 1, 57, 58, 56, 47, 52, 45]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = { c:i for i, c in enumerate(unique_chars) }\n",
    "itos = { i:c for i, c in enumerate(unique_chars) }\n",
    "\n",
    "encode = lambda some_str: [stoi[char] for char in some_str]\n",
    "decode = lambda some_str: [itos[char] for char in some_str]\n",
    "\n",
    "\n",
    "example_string = \"This is a string\"\n",
    "encode(example_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d706e85",
   "metadata": {},
   "source": [
    "# Step 1.5: Character -> Token Int -> Token Embedding Vector\n",
    "\n",
    "Step 1: making an embedding lookup table! Each row corresponds to a unique token. The number of that row is equal to the \"number\" of that token (see box above this one for an example: 'T' has token number 32, and thus the vector at row 32 IS 'T''s embedding vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e332a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text))        # turn big list of characters -> big list of token ints\n",
    "\n",
    "train_size = int(0.9 * len(data))        # train/test split: both are just long lists!\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0554a0",
   "metadata": {},
   "source": [
    "## Sidenote: chunking the training data\n",
    "\n",
    "Basically we only ever take in CHUNK_SIZE sequence of chars (taking in all of them at once would be way too hard). CHUNK_SIZE is just the __max length sequence__ we can ever predict on.\n",
    "\n",
    "Here you can see how every CHUNK_SIZE sequence is actually a bunch of training examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19c14b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the training example is tensor([], dtype=torch.int64), the label is 18.\n",
      "When the training example is tensor([18]), the label is 47.\n",
      "When the training example is tensor([18, 47]), the label is 56.\n",
      "When the training example is tensor([18, 47, 56]), the label is 57.\n",
      "When the training example is tensor([18, 47, 56, 57]), the label is 58.\n",
      "When the training example is tensor([18, 47, 56, 57, 58]), the label is 1.\n",
      "When the training example is tensor([18, 47, 56, 57, 58,  1]), the label is 15.\n",
      "When the training example is tensor([18, 47, 56, 57, 58,  1, 15]), the label is 47.\n"
     ]
    }
   ],
   "source": [
    "# Every single chunk is actually a BUNCH of training examples.\n",
    "\n",
    "chunk_size = 8\n",
    "\n",
    "for char_idx in range(chunk_size):\n",
    "    training_example = train_data[:char_idx]\n",
    "    associated_label = train_data[char_idx]\n",
    "    print(f\"When the training example is {training_example}, the label is {associated_label}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b5fc8",
   "metadata": {},
   "source": [
    "# Step 1.6: Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2431543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From train_data, pick tensor([629112, 473269, 377314,  61238]) as starting indices for training sequences.\n",
      "\n",
      "These are the block_size-length sequences starting from each of those indicies:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 58, 46, 43,  1, 49, 47, 52],\n",
      "        [ 8,  0, 21, 57,  1, 47, 58,  1],\n",
      "        [ 1, 14, 53, 50, 47, 52, 45, 40],\n",
      "        [56,  1, 39, 54, 54, 56, 53, 40]])\n",
      "\n",
      "These are the correct labels associated with each of those example tensors:\n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 46, 43,  1, 49, 47, 52, 45],\n",
      "        [ 0, 21, 57,  1, 47, 58,  1, 43],\n",
      "        [14, 53, 50, 47, 52, 45, 40, 56],\n",
      "        [ 1, 39, 54, 54, 56, 53, 40, 39]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1, 58, 46, 43,  1, 49, 47, 52],\n",
       "         [ 8,  0, 21, 57,  1, 47, 58,  1],\n",
       "         [ 1, 14, 53, 50, 47, 52, 45, 40],\n",
       "         [56,  1, 39, 54, 54, 56, 53, 40]]),\n",
       " tensor([[58, 46, 43,  1, 49, 47, 52, 45],\n",
       "         [ 0, 21, 57,  1, 47, 58,  1, 43],\n",
       "         [14, 53, 50, 47, 52, 45, 40, 56],\n",
       "         [ 1, 39, 54, 54, 56, 53, 40, 39]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4     # Batch Size = the NUMBER of sequences we forward pass, backward pass, and step with every epoch.\n",
    "block_size = 8     # Block_Size = maximum context length for a prediction.\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # for this particular batch, we want to get 4 sequences each of sequence length 32\n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))     # get 4 indexes into 'data': the indexes can only be from 0 to len(data) - block_size\n",
    "                                                                                            # this will be a 1D tensor of size (batch_size,), i.e. tensor([953063, 497175, 633405, 627354])\n",
    "    print(f\"From {split}_data, pick {random_starting_idx_of_batch} as starting indices for training sequences.\")\n",
    "    \n",
    "    # now that we have some starting indices, pick out the 32-length sequence from each of them\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "    print(\"\\nThese are the block_size-length sequences starting from each of those indicies:\")\n",
    "    print(training_sequences_tensor.shape)\n",
    "    print(training_sequences_tensor)\n",
    "    \n",
    "    \n",
    "    # now we'll get a tensor, but with all the relevant labels. Remember we're using the trick above to get more examples.\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    print(\"\\nThese are the correct labels associated with each of those example tensors:\")\n",
    "    print(labels_tensor.shape)\n",
    "    print(labels_tensor)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "    \n",
    "    \n",
    "get_batch('train')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbfaec",
   "metadata": {},
   "source": [
    "# Step 2: Forward Pass\n",
    "\n",
    "Fantastic. Now we've got the training input data and labels in a really nice, batched format. Now we'll make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e639b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From train_data, pick tensor([ 76049, 234249, 934904, 560986]) as starting indices for training sequences.\n",
      "\n",
      "These are the block_size-length sequences starting from each of those indicies:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "These are the correct labels associated with each of those example tensors:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "Batched input is originally torch.Size([4, 8]).\n",
      "After doing embedding lookup, it is torch.Size([4, 8, 65]), since we replace each int token with a 65-sized vector.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    # model has internal embedding vector lookup table based on token\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    # forward pass: foreach INT_TOKEN in input_batched, turn that into the correct embedding vector,\n",
    "    # and then replace that INT_TOKEN with the relevant embedding vector.\n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        logits = self.token_embedding_table(input_batched) \n",
    "        print(f\"\\nBatched input is originally {input_batched.shape}.\")\n",
    "        print(f\"After doing embedding lookup, it is {logits.shape}, since we replace each int token with a {len(unique_chars)}-sized vector.\")\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "out = m.forward(xb, yb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fc3c8",
   "metadata": {},
   "source": [
    "# Step 2.5: Formulate with Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efdbab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- GENERATING SEQUENCE --------\n",
      "\n",
      "sV\n",
      "vL\n",
      "ja,FsLY,wxEuS'pao3jOssyBA$zFqYTkeMk x-gQ.FzLg!iKI.egzDnyA TsTbvdgX!KpGIeJyjv,SrFF&SDt!:hwWSl.W\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # for this particular batch, we want to get 4 sequences each of sequence length 32\n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))     # get 4 indexes into 'data': the indexes can only be from 0 to len(data) - block_size\n",
    "                                                                                            # this will be a 1D tensor of size (batch_size,), i.e. tensor([953063, 497175, 633405, 627354])\n",
    "#     print(f\"From {split}_data, pick {random_starting_idx_of_batch} as starting indices for training sequences.\")\n",
    "    \n",
    "    # now that we have some starting indices, pick out the 32-length sequence from each of them\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "#     print(\"\\nThese are the block_size-length sequences starting from each of those indicies:\")\n",
    "#     print(training_sequences_tensor.shape)\n",
    "#     print(training_sequences_tensor)\n",
    "    \n",
    "    \n",
    "    # now we'll get a tensor, but with all the relevant labels. Remember we're using the trick above to get more examples.\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "#     print(\"\\nThese are the correct labels associated with each of those example tensors:\")\n",
    "#     print(labels_tensor.shape)\n",
    "#     print(labels_tensor)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    # model has internal embedding vector lookup table based on token\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    # forward pass: foreach INT_TOKEN in input_batched, turn that into the correct embedding vector,\n",
    "    # and then replace that INT_TOKEN with the relevant embedding vector.\n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        logits = self.token_embedding_table(input_batched)           # replace all INT_TOKENS with vector embeddings\n",
    "        \n",
    "        \n",
    "        if target_batched is None:\n",
    "            loss = None                                              # If no target is provided,\n",
    "        else:\n",
    "            batch_size, sequence_length, embedding_dim = logits.shape    \n",
    "            logits = logits.view(batch_size * sequence_length, embedding_dim)   # stack ALL embeddings in a SINGLE LIST\n",
    "\n",
    "            targets = target_batched.view(batch_size * sequence_length)    # do same for target INT_TOKENS: there are 4 sequences per batch, with each sequence having 8 characters\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)        # provide a tensor of embeddings, and a tensor of targets, and it will find the loss\n",
    "\n",
    "        ### --------------------------------------\n",
    "        \n",
    "#         print(\"\\nUnstackify the logits tensor:\")\n",
    "#         print(logits.shape)\n",
    "#         print(logits)\n",
    "        \n",
    "#         if target_batched is not None:\n",
    "#             print(\"\\nUnstackify the targets tensor:\")\n",
    "#             print(targets.shape)\n",
    "#             print(targets)\n",
    "#             print(f\"\\nThe logit and associated target can now be matched 1-to-1. The cross-entropy loss is {loss:.4f}.\")\n",
    "        ### --------------------------------------\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, starting_sequence_tensor, max_new_tokens):\n",
    "        \n",
    "        \n",
    "        print(\"\\n-------- GENERATING SEQUENCE --------\")\n",
    "        \n",
    "        # repeat for the number of new tokens you want...\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(starting_sequence_tensor, None)        # forward pass our input sequence: basically just convering INT_TOKENS to associated embedding vector\n",
    "#             print(logits.shape)                                                # batch_size  x  single_batch_sequence_length  x  embedding_dimension\n",
    "            logits = logits[:, -1, :]                                    # from our logits tensor (list of embedding vectors), we only care about the very LAST one in the timestep\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)     # convert logits (of vocab-size) to probability distribution             \n",
    "            generated_int_token = torch.multinomial(probs, num_samples=1)        # sample from that probability distribution\n",
    "            starting_sequence_tensor = torch.cat((starting_sequence_tensor, generated_int_token), dim=1)        # append that new token\n",
    "#             print(f\"starting_sequence_tensor shape: {starting_sequence_tensor.shape}\")\n",
    "            \n",
    "        return starting_sequence_tensor\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "# out, loss = m.forward(xb, yb)\n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)\n",
    "\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f959201",
   "metadata": {},
   "source": [
    "# Step 3: Training\n",
    "\n",
    "Great, now we can do a forward pass that does input_int_token_sequence -> embedding_vector_sequence/logits. We can also convert those logits to a probability distribution via a softmax, and then sample from that distribution to get a new token, which we can append back onto the original input_int_token_sequence and continue onwards.\n",
    "\n",
    "Now let's train our model parameters (which is just m.parameters() - remember that m is a nn.Module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e28a39b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is: 2.5045759677886963\n",
      "loss at epoch 2000 is: 2.4128713607788086\n",
      "loss at epoch 4000 is: 2.4660370349884033\n",
      "loss at epoch 6000 is: 2.4909756183624268\n",
      "loss at epoch 8000 is: 2.3688089847564697\n",
      "\n",
      "-------- GENERATING SEQUENCE --------\n",
      "\n",
      "Yourete fay MI RIOPUCap t waug whassely sy e msbe shes, d th, h youre w ag mur ore irt\n",
      "Ano and t wis\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batched, label_batched = get_batch('train')\n",
    "    \n",
    "    logits, loss = m.forward(input_batched, label_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)                     # RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'loss at epoch {epoch} is: {loss.item()}')\n",
    "\n",
    "    \n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n",
    "\n",
    "\n",
    "# pretty good! The only issue is that we're only using the last logit-embedding-vector to predict the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacfba8",
   "metadata": {},
   "source": [
    "# SECTION II: Self-Attention\n",
    "\n",
    "How do we get tokens to talk to each other?\n",
    "\n",
    "- Don't want future tokens to communicate to past tokens. You're trying to PREDICT the future given the past! It's not like given the past AND the future, predict the intermediate.\n",
    "\n",
    "- Naive Solution 1: if you're token 5, take tokens 1-4, average them up, and the resulting vector is a \"context\" summarizing token 5 in the CONTEXT of what came before.\n",
    "    - Issue: Averaging/sum is extremely lossy: you've lost spatial/positional information.\n",
    "    \n",
    "    \n",
    "Let's do this naive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bd9eb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size, seq_length, embedding_size = 4, 8, 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_length, embedding_size)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c575e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication 1: want x[b, t] = mean of x[b, i] where i <= t\n",
    "mean_embeddings = torch.zeros((batch_size, seq_length, embedding_size))\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    for token_idx in range(seq_length):\n",
    "        prev_embeddings = x[batch, : token_idx+1]\n",
    "        mean_embeddings[batch, token_idx] = torch.mean(prev_embeddings, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aad02a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72be49e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice: each subsequence timestep \"incorporates\" information from all the previous timesteps! We just take an avg!\n",
    "mean_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fefd081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b = \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = \n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "\n",
      " Notice how multiplying by a lower-triangular ones matrix allows you to incrementally add each vector of b!\n"
     ]
    }
   ],
   "source": [
    "# MATHEMATICAL TRICK TO MAKING THE ABOVE CODE EFFICIENT VIA MATRIX MULTIPLICATIONS\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "a = torch.tril(a)    # convert a to lower-triangular\n",
    "\n",
    "\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"c = \")\n",
    "print(c)\n",
    "\n",
    "print(\"\\n Notice how multiplying by a lower-triangular ones matrix allows you to incrementally add each vector of b!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d96c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b = \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = \n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "\n",
      "Notice how a's rows sum to 1! This lets you compute the incremental-acerage as described!\n",
      "Think through a @ b, and notice how you're incrementally averaging all the previous rows!\n"
     ]
    }
   ],
   "source": [
    "# You can thus easily compute the average by making the row-elems of a to sum to 1!\n",
    "# MATHEMATICAL TRICK TO MAKING THE ABOVE CODE EFFICIENT VIA MATRIX MULTIPLICATIONS\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "a = torch.tril(a)    # convert a to lower-triangular\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"c = \")\n",
    "print(c)\n",
    "\n",
    "print(\"\\nNotice how a's rows sum to 1! This lets you compute the incremental-acerage as described!\\nThink through a @ b, and notice how you're incrementally averaging all the previous rows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c160022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embeddings =\n",
      "tensor([[ 1.2946,  0.2227],\n",
      "        [-1.2924,  0.1689],\n",
      "        [-0.8326, -0.8129],\n",
      "        [ 0.9700, -0.6758]])\n",
      "averaging_matrix =\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "averaged_input_embeddings =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2946e+00,  2.2267e-01],\n",
       "        [ 1.1061e-03,  1.9577e-01],\n",
       "        [-2.7680e-01, -1.4047e-01],\n",
       "        [ 3.4909e-02, -2.7429e-01]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do it on our larger toy example\n",
    "# batch_size, seq_length, embedding_size = 4, 8, 2\n",
    "B, T, C = 4, 4, 2\n",
    "\n",
    "input_embeddings = torch.randn(B, T, C)   # within a single batch, our goal is to get the \"incremental averaging\" of all the previous vectors\n",
    "\n",
    "\n",
    "averaging_matrix = torch.ones(T, T)       # the averaging matrix is seq_length x seq_length\n",
    "averaging_matrix = torch.tril(averaging_matrix)\n",
    "averaging_matrix = averaging_matrix / torch.sum(averaging_matrix, 1, keepdim=True)\n",
    "\n",
    "\n",
    "print(\"input_embeddings =\")\n",
    "print(input_embeddings[0])\n",
    "\n",
    "print(\"averaging_matrix =\")\n",
    "print(averaging_matrix)\n",
    "\n",
    "averaged_input_embeddings = averaging_matrix @ input_embeddings   # little strange, but this applies the matrix to each batch\n",
    "                                                                  # (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) -> (B, T, C)    :     averaging matrix is broadcast along batch dim\n",
    "\n",
    "print(\"averaged_input_embeddings =\")\n",
    "averaged_input_embeddings[0]\n",
    "\n",
    "\n",
    "# AVERAGING MATRIX IS A WEIGHT MATRIX! TOKEN GETS INFO ONLY FROM TOKENS PREVIOUS TO IT \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18d52c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small trick: Note that for softmax, we actually set the upper triangle of averaging_matrix to '-inf': so that AFTER\n",
    "# the softmax those entries get turned to 0.\n",
    "\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "\n",
    "weights_matrix = torch.zeros((T, T))\n",
    "weights_matrix = weights_matrix.masked_fill(tril == 0, float('-inf'))   # tokens from the future cna't communicate with the past\n",
    "weights_matrix = F.softmax(weights_matrix, dim=-1)\n",
    "\n",
    "weights_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c759b8",
   "metadata": {},
   "source": [
    "__KEY TAKEAWAY__: Given some list of embeddings, you can do _weighted aggregations_ of your *past* sequence embeddings via _matrix multiplication_ with a lower-triangular matrix, where the elements in the lower-triangular part tell you how much each element fuses into the current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959b45a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n",
      "Attention weights for a SINGLE batch: notice how it's not equally weighted anymore!\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "In this specific example, looking at the last row, the 8th token sees the 7th and 4th token (0.2423 and 0.2297 respectively) as highly relevant w.r.t. their query and key vectors being aligned.\n",
      "\n",
      "Shape of the output:\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# SELF-ATTENTION FULL CODE\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32        # batch, time, channels/embedding size\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "\n",
    "# this code does a rolling average of previous embeddings via a low-triangular matrix\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ x\n",
    "\n",
    "print(\"OG weights:\\n\", weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ABSOLUTELY CRITICAL:\n",
    "# Different tokens find other tokens more or less interesting! Different past tokens should have different weights, currently OG weights just weights all past tokens equally!\n",
    "\n",
    "# Soln: for each embedding in my sequence, emit a query (what I'm looking for) and key (what do I contain) vectors.\n",
    "# The dot product btwn key and query IS my weight! If they're more aligned, I learn MORE from that specific token!\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)        # (B, T, 16)\n",
    "q = query(x)      # (B, T, 16)\n",
    "v = value(x)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1)    # (B, T, 16) @ (B, 16, T)    --->    (B, T, T)     :     This is our better weight matrix!\n",
    "# weights = torch.zeros((T, T))      # no longer zeros...\n",
    "\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))        # we make the future weights all -inf so they become 0 during the softmax\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ v\n",
    "\n",
    "print(\"\\nAttention weights for a SINGLE batch: notice how it's not equally weighted anymore!\\n\", weights[0])\n",
    "\n",
    "print(\"\\nIn this specific example, looking at the last row, the 8th token sees the 7th and 4th token (0.2423 and 0.2297 respectively) as highly relevant w.r.t. their query and key vectors being aligned.\")\n",
    "\n",
    "\n",
    "print(\"\\nShape of the output:\")\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bf1eb",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "    - Graph of nodes that get information from itself + all nodes that connect to it. Communication in a directed graph!\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "    - Vector gets a thing added to it that just denotes its specific position\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "    - BATCHES ALL INDEPENDENT\n",
    "\n",
    "\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "    - the masked_fill tril lines prevent future from affecting past: in a sentiment analysis app you WOULD want the future to affect the past: so delete those lines with the tril.\n",
    "    - \"decoder\" in the sense that this triangular masking lets us do autoregressive language modeling\n",
    "    - attention is just ARBITRARY COMMUNICATION ACROSS VECTORS/NODES\n",
    "\n",
    "\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "    - \"cross-attention\" means the keys and values come from ELSEWHERE\n",
    "\n",
    "\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n",
    "    - basically SOFTMAX on LARGE NUMBERS will put a LOT more weight on the largest node: so we just divide by this term to prevent this bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd202e6",
   "metadata": {},
   "source": [
    "# SECTION III: Let's add this self-attention block to the language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c8b523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is: 4.245684623718262\n",
      "loss at epoch 2000 is: 2.904353618621826\n",
      "loss at epoch 4000 is: 2.6421000957489014\n",
      "loss at epoch 6000 is: 2.5174005031585693\n",
      "loss at epoch 8000 is: 2.295194387435913\n",
      "\n",
      "-------- GENERATING SEQUENCE --------\n",
      "\n",
      "AnggG omme owine qme se y'se dk blot CGo I gje:\n",
      "ag spalpoudrd sis tl bessod ts fo des?\n",
      "UNave ang. Lal cin IS;\n",
      "do tn bilbiat yout int, Wiod throur,\n",
      "Forsotha iut,\n",
      "in piserle ss we, be Lis od ot thinudlint nt lorth siconot, wharoulls orde to me\n",
      "Bwres nto trim LI ay, tis akine be fe dn.\n",
      "AGo owre uhet I ENEREOR:\n",
      "Hhot the tcoma! his IZDUI Vorwn te wo tl an'shas dramecaswilleh the yof sot pre fad et the the poretro?\n",
      "\n",
      "SAn mend, thy's he tl!\n",
      "\n",
      "sof sth fo on\n",
      "And ond baethises ls m hilsho tlasadd snwifoun n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "\n",
    "\n",
    "embedding_dim = 32\n",
    "max_context_length = 8\n",
    "block_size = max_context_length\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Linear layers for query, key, and value\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False) \n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, input_sequence_batched):\n",
    "        B, T, C = input_sequence_batched.shape   # B: batch size, T: sequence length, C: embedding dim\n",
    "        \n",
    "        # Compute keys, queries, and values\n",
    "        keys_batched = self.key(input_sequence_batched)       # Shape: (B, T, head_size)\n",
    "        queries_batched = self.query(input_sequence_batched)  # Shape: (B, T, head_size)\n",
    "        values_batched = self.value(input_sequence_batched)   # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        wei = queries_batched @ keys_batched.transpose(-2, -1) * (C ** -0.5)  # Scale by sqrt(d_k)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))          # Apply mask\n",
    "        wei = F.softmax(wei, dim=-1)                                         # Normalize weights\n",
    "        \n",
    "        # Apply attention\n",
    "        out = wei @ values_batched  # Shape: (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)        \n",
    "        self.position_embedding_table = nn.Embedding(max_context_length, embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.sa_head = Head(embedding_dim)                     # convert vector_embedding -> attentionified_vector_embedding\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)    # convert vector_embedding -> vocab_size (REAL logits)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        B, T = input_batched.shape\n",
    "        \n",
    "        sequence_token_embeddings = self.token_embedding_table(input_batched)\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T))   # 0....T-1 for a (T, embedding_dim) matrix\n",
    "        \n",
    "        x = sequence_token_embeddings + positional_embedding                    # this is how you add positional infomration into the sequence\n",
    "        \n",
    "        x = self.sa_head(x)         # Apply attention block\n",
    "        logits = self.lm_head(x)    # Decode back to vocab-sized logits (right before softmaxxing and sampling a word)\n",
    "        \n",
    "        \n",
    "        if target_batched is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, embedding_dim = logits.shape    \n",
    "            logits = logits.view(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "            targets = target_batched.view(batch_size * sequence_length) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, starting_sequence_tensor, max_new_tokens):\n",
    "        \n",
    "        \n",
    "        print(\"\\n-------- GENERATING SEQUENCE --------\")\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            context = starting_sequence_tensor[:, -block_size:]   # crop \"input context\" to just the last block_size tokens... next token depends on last ~8 tokens (otherwise the input dimensions are off)\n",
    "            \n",
    "            logits, loss = self.forward(context, None)        # forward pass our input sequence: basically just convering INT_TOKENS to associated embedding vector\n",
    "            logits = logits[:, -1, :]                                          # from our logits tensor (list of embedding vectors), we only care about the very LAST one in the timestep\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                                  # convert logits (of vocab-size) to probability distribution             \n",
    "            generated_int_token = torch.multinomial(probs, num_samples=1)      # sample from that probability distribution\n",
    "            starting_sequence_tensor = torch.cat((starting_sequence_tensor, generated_int_token), dim=1)        # append that new token\n",
    "            \n",
    "        return starting_sequence_tensor\n",
    "        \n",
    "        \n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batched, label_batched = get_batch('train')\n",
    "    \n",
    "    logits, loss = m.forward(input_batched, label_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)                     # RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'loss at epoch {epoch} is: {loss.item()}')\n",
    "\n",
    "    \n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 500)\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n",
    "\n",
    "\n",
    "# looks better! attention clearly communicating SOMETHING clearly....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275572f",
   "metadata": {},
   "source": [
    "## Section 3.5: Multiheaded Attention\n",
    "\n",
    "Adding multiple heads is very simple. We'll also add a feedforward layer at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a53d28a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is: 4.12933349609375\n",
      "loss at epoch 2000 is: 2.2637226581573486\n",
      "loss at epoch 4000 is: 2.456537961959839\n",
      "loss at epoch 6000 is: 2.572815179824829\n",
      "loss at epoch 8000 is: 2.2864067554473877\n",
      "\n",
      "-------- GENERATING SEQUENCE --------\n",
      "\n",
      "NUCAIORD:\n",
      "Fet\n",
      "Aplodce ithe hathe one had,\n",
      "Thh\n",
      "Ford my lelly of thilll\n",
      "Flle ate, s then wess writh he lot, woo nou benat con len:\n",
      "truse ef thereat we alraid thin soull, wath,\n",
      "Adonce sar soow yoor the on ath wingt of tthe tor; thick rist; vill Cave sad;\n",
      "And: wing?\n",
      "\n",
      "Ande ofout pouts;\n",
      "He worave wile,\n",
      "At; he nousse to froorous stid atin, on!\n",
      "Sit be, the flois bau de exher wick.:\n",
      "Do,\n",
      "Iss thillid Vard vearte this, he fou.\n",
      "\n",
      "Thaceay miom gea you thie, fored, ove\n",
      "The derin sess perbieve whiche ly forme to\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "\n",
    "\n",
    "embedding_dim = 32\n",
    "max_context_length = 8\n",
    "block_size = max_context_length\n",
    "\n",
    "\"\"\"SUBMODULE 1: ATTENTION HEAD/FUNCTION\"\"\"\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Linear layers for query, key, and value\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False) \n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, input_sequence_batched):\n",
    "        B, T, C = input_sequence_batched.shape   # B: batch size, T: sequence length, C: embedding dim\n",
    "        \n",
    "        # Compute keys, queries, and values\n",
    "        keys_batched = self.key(input_sequence_batched)       # Shape: (B, T, head_size)\n",
    "        queries_batched = self.query(input_sequence_batched)  # Shape: (B, T, head_size)\n",
    "        values_batched = self.value(input_sequence_batched)   # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        wei = queries_batched @ keys_batched.transpose(-2, -1) * (C ** -0.5)  # Scale by sqrt(d_k)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))          # Apply mask\n",
    "        wei = F.softmax(wei, dim=-1)                                         # Normalize weights\n",
    "        \n",
    "        # Apply attention\n",
    "        out = wei @ values_batched  # Shape: (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "\"\"\"SUBMODULE 2: MULTIPLE HEADS OF ATTENTION\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Run multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])    # basically get a LIST of heads\n",
    "    \n",
    "    def forward(self, input_sequence):\n",
    "        return torch.cat([h.forward(input_sequence) for h in self.heads], dim=-1)    # for every head, call forward() and concat the attention-ified results!\n",
    "        \n",
    "        \n",
    "\n",
    "\"\"\"SUBMODULE 3: SIMPLE FEEDFORWARD MLP LAYER\"\"\"\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_sequence):\n",
    "        return self.mlp_layer(input_sequence)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)        \n",
    "        self.position_embedding_table = nn.Embedding(max_context_length, embedding_dim)\n",
    "        \n",
    "        \n",
    "#         self.sa_head = Head(embedding_dim)                     # convert vector_embedding -> attentionified_vector_embedding\n",
    "        self.sa_head = MultiHeadAttention(4, embedding_dim // 4)   \n",
    "        self.ffw = FeedForward(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)    # convert vector_embedding -> vocab_size (REAL logits)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        B, T = input_batched.shape\n",
    "        \n",
    "        sequence_token_embeddings = self.token_embedding_table(input_batched)\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T))   # 0....T-1 for a (T, embedding_dim) matrix\n",
    "        \n",
    "        x = sequence_token_embeddings + positional_embedding                    # (B,T,C) - this is how you add positional infomration into the sequence\n",
    "        \n",
    "        x = self.sa_head(x)         # (B,T,C) - Apply attention block\n",
    "        x = self.ffw(x)            # (B,T,C) - FFWD get applied to EVERY SINGLE VECTOR in a sequence for EVERY SINGLE BATCH!\n",
    "        logits = self.lm_head(x)    # Decode back to vocab-sized logits (right before softmaxxing and sampling a word)\n",
    "        \n",
    "        \n",
    "        if target_batched is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, embedding_dim = logits.shape    \n",
    "            logits = logits.view(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "            targets = target_batched.view(batch_size * sequence_length) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, starting_sequence_tensor, max_new_tokens):\n",
    "        \n",
    "        \n",
    "        print(\"\\n-------- GENERATING SEQUENCE --------\")\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            context = starting_sequence_tensor[:, -block_size:]   # crop \"input context\" to just the last block_size tokens... next token depends on last ~8 tokens (otherwise the input dimensions are off)\n",
    "            \n",
    "            logits, loss = self.forward(context, None)        # forward pass our input sequence: basically just convering INT_TOKENS to associated embedding vector\n",
    "            logits = logits[:, -1, :]                                          # from our logits tensor (list of embedding vectors), we only care about the very LAST one in the timestep\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                                  # convert logits (of vocab-size) to probability distribution             \n",
    "            generated_int_token = torch.multinomial(probs, num_samples=1)      # sample from that probability distribution\n",
    "            starting_sequence_tensor = torch.cat((starting_sequence_tensor, generated_int_token), dim=1)        # append that new token\n",
    "            \n",
    "        return starting_sequence_tensor\n",
    "        \n",
    "        \n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batched, label_batched = get_batch('train')\n",
    "    \n",
    "    logits, loss = m.forward(input_batched, label_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)                     # RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'loss at epoch {epoch} is: {loss.item()}')\n",
    "\n",
    "    \n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 500)\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n",
    "\n",
    "\n",
    "# looks better! attention clearly communicating SOMETHING clearly....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063e87b",
   "metadata": {},
   "source": [
    "## Section 3.6: RESIDUAL SKIP CONNECTIONS\n",
    "\n",
    "Our loss is increasing! Now we need to add more of these (multi-head attention, feedforward) BLOCKs!\n",
    "\n",
    "However, now we run into DEEP neural nets with a lot of blocks, i.e. vanishing gradient problem or something.\n",
    "\n",
    "Solution: __SKIP CONNECTIONS!__ \n",
    "\n",
    "x_{t} ------------>  x_{t+1}   +   (x_{t} passed through the block)\n",
    "\n",
    "\n",
    "Gradients can both flow unimpeded to the original input, AS WELL AS the block itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "109c6e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is: 4.794428825378418\n",
      "loss at epoch 2000 is: 2.2961947917938232\n",
      "loss at epoch 4000 is: 1.5466394424438477\n",
      "loss at epoch 6000 is: 1.9636958837509155\n",
      "loss at epoch 8000 is: 2.4651312828063965\n",
      "\n",
      "-------- GENERATING SEQUENCE --------\n",
      "\n",
      "\n",
      "For EENIO:\n",
      "Then:\n",
      "And me; didurcie she\n",
      "ceray opiin ghisce,\n",
      "HinnET I thinine! tee me.\n",
      "\n",
      "KING ANINTOLARARD\n",
      "And the anin epeet-men hemly and with ine time the.\n",
      "\n",
      "LURAMENT:\n",
      "JoN in he gipn trou haciuldie;\n",
      "As the you sbro seploon:\n",
      "We fort ent ridegd,\n",
      "In KING Eice theaple\n",
      "come atak?\n",
      "\n",
      "S: moree' Abt and alloy ling in me aremose lisse fressesece me\n",
      "not I o, it golt; hom and, what You to an more a conte of a nothere tor, Go?\n",
      "\n",
      "COYENTE:\n",
      "This on of nuen prine, I that, I ald his by sers grefesss sinth's in.\n",
      "\n",
      "APE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "\n",
    "\n",
    "embedding_dim = 32\n",
    "max_context_length = 8\n",
    "block_size = max_context_length\n",
    "\n",
    "\"\"\"SUBMODULE 1: ATTENTION HEAD/FUNCTION\"\"\"\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Linear layers for query, key, and value\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False) \n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, input_sequence_batched):\n",
    "        B, T, C = input_sequence_batched.shape   # B: batch size, T: sequence length, C: embedding dim\n",
    "        \n",
    "        # Compute keys, queries, and values\n",
    "        keys_batched = self.key(input_sequence_batched)       # Shape: (B, T, head_size)\n",
    "        queries_batched = self.query(input_sequence_batched)  # Shape: (B, T, head_size)\n",
    "        values_batched = self.value(input_sequence_batched)   # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        wei = queries_batched @ keys_batched.transpose(-2, -1) * (C ** -0.5)  # Scale by sqrt(d_k)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))          # Apply mask\n",
    "        wei = F.softmax(wei, dim=-1)                                         # Normalize weights\n",
    "        \n",
    "        # Apply attention\n",
    "        out = wei @ values_batched  # Shape: (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "\"\"\"SUBMODULE 2: MULTIPLE HEADS OF ATTENTION\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Run multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])    # basically get a LIST of heads\n",
    "        \n",
    "        self.proj = nn.Linear(embedding_size, embedding_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        out = torch.cat([h.forward(input_sequence) for h in self.heads], dim=-1)    # for every head, call forward() and concat the attention-ified results!\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out \n",
    "        \n",
    "\n",
    "\"\"\"SUBMODULE 3: SIMPLE FEEDFORWARD MLP LAYER\"\"\"\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),  # \"projection layer\" back into residual pathway(?????)\n",
    "            \n",
    "            # (need that \"projection\" layer back since FFW is 4 * embedding_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_sequence):\n",
    "        return self.mlp_layer(input_sequence)\n",
    "        \n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embedding_size // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ffw = FeedForward(embedding_size)\n",
    "    \n",
    "    def forward(self, input_sequence):\n",
    "#         input_sequence = self.sa(input_sequence)        # PAIR THESE SA AND FEEDFORWARD LAYERS INTO A SINGLE \"BLOCK\" FUNCTION!\n",
    "#         input_sequence = self.ffw(input_sequence)\n",
    "        \n",
    "        \n",
    "        # \"RESIDUAL\"/SKIP CONNECTIONS!\n",
    "        \n",
    "        input_sequence = input_sequence + self.sa(input_sequence)        # PAIR THESE SA AND FEEDFORWARD LAYERS INTO A SINGLE \"BLOCK\" FUNCTION!\n",
    "        input_sequence = input_sequence + self.ffw(input_sequence)\n",
    "        \n",
    "        \n",
    "        return input_sequence\n",
    "        \n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)        \n",
    "        self.position_embedding_table = nn.Embedding(max_context_length, embedding_dim)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(embedding_size, num_heads = 4),\n",
    "            Block(embedding_size, num_heads = 4),\n",
    "            Block(embedding_size, num_heads = 4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        B, T = input_batched.shape\n",
    "        \n",
    "        sequence_token_embeddings = self.token_embedding_table(input_batched)\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T))   # 0....T-1 for a (T, embedding_dim) matrix\n",
    "        \n",
    "        x = sequence_token_embeddings + positional_embedding                    # (B,T,C) - this is how you add positional infomration into the sequence\n",
    "        \n",
    "        x = self.blocks(x)          # run input vectors through that collection of blocks...\n",
    "        logits = self.lm_head(x)    # Decode back to vocab-sized logits (right before softmaxxing and sampling a word)\n",
    "        \n",
    "        \n",
    "        if target_batched is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, embedding_dim = logits.shape    \n",
    "            logits = logits.view(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "            targets = target_batched.view(batch_size * sequence_length) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, starting_sequence_tensor, max_new_tokens):\n",
    "        \n",
    "        \n",
    "        print(\"\\n-------- GENERATING SEQUENCE --------\")\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            context = starting_sequence_tensor[:, -block_size:]   # crop \"input context\" to just the last block_size tokens... next token depends on last ~8 tokens (otherwise the input dimensions are off)\n",
    "            \n",
    "            logits, loss = self.forward(context, None)        # forward pass our input sequence: basically just convering INT_TOKENS to associated embedding vector\n",
    "            logits = logits[:, -1, :]                                          # from our logits tensor (list of embedding vectors), we only care about the very LAST one in the timestep\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                                  # convert logits (of vocab-size) to probability distribution             \n",
    "            generated_int_token = torch.multinomial(probs, num_samples=1)      # sample from that probability distribution\n",
    "            starting_sequence_tensor = torch.cat((starting_sequence_tensor, generated_int_token), dim=1)        # append that new token\n",
    "            \n",
    "        return starting_sequence_tensor\n",
    "        \n",
    "        \n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "embedding_size = 32\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batched, label_batched = get_batch('train')\n",
    "    \n",
    "    logits, loss = m.forward(input_batched, label_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)                     # RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'loss at epoch {epoch} is: {loss.item()}')\n",
    "\n",
    "    \n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 500)\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n",
    "\n",
    "\n",
    "# looks better! attention clearly communicating SOMETHING clearly....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150a9c5",
   "metadata": {},
   "source": [
    "# Section 3.7: More \"deep\" network optimizations\n",
    "\n",
    "Residual is the \"add\" part of \"add & norm\": norm refers to layer-normalization. \n",
    "\n",
    "Layer-normalization: basically make each row a gaussian with some mean or some shit? Read the paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02b5972b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is: 4.338974475860596\n",
      "loss at epoch 2000 is: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 196\u001b[0m\n\u001b[1;32m    194\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward(input_batched, label_batched)\n\u001b[1;32m    195\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)                     \u001b[38;5;66;03m# RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/projects/smolgrad/smol/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/smolgrad/smol/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/smolgrad/smol/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "\n",
    "\"\"\"SUBMODULE 1: ATTENTION HEAD/FUNCTION\"\"\"\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Linear layers for query, key, and value\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False) \n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, input_sequence_batched):\n",
    "        B, T, C = input_sequence_batched.shape   # B: batch size, T: sequence length, C: embedding dim\n",
    "        \n",
    "        # Compute keys, queries, and values\n",
    "        keys_batched = self.key(input_sequence_batched)       # Shape: (B, T, head_size)\n",
    "        queries_batched = self.query(input_sequence_batched)  # Shape: (B, T, head_size)\n",
    "        values_batched = self.value(input_sequence_batched)   # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        wei = queries_batched @ keys_batched.transpose(-2, -1) * (C ** -0.5)  # Scale by sqrt(d_k)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))          # Apply mask\n",
    "        wei = F.softmax(wei, dim=-1)                                         # Normalize weights\n",
    "        \n",
    "        # Apply attention\n",
    "        out = wei @ values_batched  # Shape: (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "\"\"\"SUBMODULE 2: MULTIPLE HEADS OF ATTENTION\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Run multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])    # basically get a LIST of heads\n",
    "        \n",
    "        self.proj = nn.Linear(embedding_size, embedding_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        out = torch.cat([h.forward(input_sequence) for h in self.heads], dim=-1)    # for every head, call forward() and concat the attention-ified results!\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out \n",
    "        \n",
    "\n",
    "\"\"\"SUBMODULE 3: SIMPLE FEEDFORWARD MLP LAYER\"\"\"\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),  # \"projection layer\" back into residual pathway(?????)\n",
    "            \n",
    "            nn.Dropout(dropout),   # ALSO  ADDING SOME DROPOUT AS WELL!\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_sequence):\n",
    "        return self.mlp_layer(input_sequence)\n",
    "        \n",
    "\n",
    "        \n",
    "# TRANSFORMER \"DECODER\" BLOCK: WITH MULTIHEADATTENTION, FEEDFORWARD, LAYER NORMS, AND RESIDUAL/SKIP CONNECTIONS!\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embedding_size // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ffw = FeedForward(embedding_size)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)     # ADD EXTRA LAYER NORMALIZATION: LAYER NORM ALSO HAS TRAINABLE PARAMS\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        # APPLY LAYERNORMS BEFORE FEEDING INTO THE ATTENTION/FFW BLOCKS!\n",
    "        input_sequence = input_sequence + self.sa(self.ln1(input_sequence))        # PAIR THESE SA AND FEEDFORWARD LAYERS INTO A SINGLE \"BLOCK\" FUNCTION!\n",
    "        input_sequence = input_sequence + self.ffw(self.ln2(input_sequence))\n",
    "        \n",
    "        \n",
    "        return input_sequence\n",
    "        \n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)        \n",
    "        self.position_embedding_table = nn.Embedding(max_context_length, embedding_dim)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(embedding_size, num_heads = 4),\n",
    "            Block(embedding_size, num_heads = 4),\n",
    "            Block(embedding_size, num_heads = 4),    # RUN AND TRAIN ON 3 BLOCKS...\n",
    "            nn.LayerNorm(embedding_size),            # AND LAYERNORM AT THE VERY END!\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        B, T = input_batched.shape\n",
    "        \n",
    "        sequence_token_embeddings = self.token_embedding_table(input_batched)\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T))   # 0....T-1 for a (T, embedding_dim) matrix\n",
    "        \n",
    "        x = sequence_token_embeddings + positional_embedding                    # (B,T,C) - this is how you add positional infomration into the sequence\n",
    "        \n",
    "        x = self.blocks(x)          # run input vectors through that collection of blocks...\n",
    "        logits = self.lm_head(x)    # Decode back to vocab-sized logits (right before softmaxxing and sampling a word)\n",
    "        \n",
    "        \n",
    "        if target_batched is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, embedding_dim = logits.shape    \n",
    "            logits = logits.view(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "            targets = target_batched.view(batch_size * sequence_length) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, starting_sequence_tensor, max_new_tokens):\n",
    "        \n",
    "        \n",
    "        print(\"\\n-------- GENERATING SEQUENCE --------\")\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            context = starting_sequence_tensor[:, -block_size:]   # crop \"input context\" to just the last block_size tokens... next token depends on last ~8 tokens (otherwise the input dimensions are off)\n",
    "            \n",
    "            logits, loss = self.forward(context, None)        # forward pass our input sequence: basically just convering INT_TOKENS to associated embedding vector\n",
    "            logits = logits[:, -1, :]                                          # from our logits tensor (list of embedding vectors), we only care about the very LAST one in the timestep\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                                  # convert logits (of vocab-size) to probability distribution             \n",
    "            generated_int_token = torch.multinomial(probs, num_samples=1)      # sample from that probability distribution\n",
    "            starting_sequence_tensor = torch.cat((starting_sequence_tensor, generated_int_token), dim=1)        # append that new token\n",
    "            \n",
    "        return starting_sequence_tensor\n",
    "        \n",
    "        \n",
    "embedding_dim = 64\n",
    "max_context_length = 256\n",
    "block_size = max_context_length\n",
    "learning_rate = 3e04\n",
    "dropout = 0.2\n",
    "\n",
    "embedding_size = 32\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batched, label_batched = get_batch('train')\n",
    "    \n",
    "    logits, loss = m.forward(input_batched, label_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)                     # RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'loss at epoch {epoch} is: {loss.item()}')\n",
    "\n",
    "    \n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 500)\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n",
    "\n",
    "\n",
    "# looks better! attention clearly communicating SOMETHING clearly....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29477725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0e377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smol)",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
