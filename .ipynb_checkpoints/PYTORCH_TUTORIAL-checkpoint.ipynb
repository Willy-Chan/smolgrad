{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cd46f1-eac2-4bbb-adc3-8a4139d5ae82",
   "metadata": {},
   "source": [
    "# BIG PYTORCH LOOKUP SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ce7a43-2e62-4b09-880a-8d551ea72ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bbfa4-8a18-47ca-b4e3-556e67dac750",
   "metadata": {},
   "source": [
    "# Every TensorOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637c2faf-6030-438e-9b87-61cf3f14c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1.1110, 2.0000, 3.0000],\n",
      "        [4.0000, 5.0000, 6.0000]])\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = [[1, 2, 3], [4, 5, 6]]\n",
    "data = torch.tensor(list_of_lists)\n",
    "print(data)\n",
    "\n",
    "list_of_lists = [[1.111, 2, 3], [4, 5, 6]]\n",
    "data2 = torch.tensor(list_of_lists, dtype=torch.float32)\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c319d8e0-5889-45ca-822f-953990ab3ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# ALL SPECIAL TENSORS YOU NEED TO KNOW\n",
    "zeros = torch.zeros(2, 5)\n",
    "print(zeros)\n",
    "ones = torch.ones(2, 5)\n",
    "print(ones)\n",
    "rr = torch.arange(1, 10)     # arange always gives you a 1D TENSOR of the range (start_inc, end_exc)!!!\n",
    "print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96897bd6-14c1-4515-897a-38c13a849c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19., 20., 21.],\n",
      "        [22., 23., 24., 25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32., 33., 34., 35.]])\n",
      "tensor([ 28.,  77., 126., 175., 224.])\n",
      "tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
      "tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n",
      "tensor(630.)\n",
      "tensor([[ 1.0000,  2.2000,  9.6000],\n",
      "        [ 4.0000, -7.2000,  6.3000]])\n",
      "tensor([ 2.5000, -2.5000,  7.9500])\n",
      "tensor([4.2667, 1.0333])\n"
     ]
    }
   ],
   "source": [
    "# ALL IMPORTANT OPS\n",
    "# ELEM WISE OPS\n",
    "print(rr + 2)\n",
    "print(rr * 2)\n",
    "\n",
    "# MATMUL OR @\n",
    "print(zeros @ ones.T)\n",
    "print(zeros.matmul(ones.T))\n",
    "\n",
    "# VECTORIZED OPERATIONS\n",
    "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
    "print(data)\n",
    "\n",
    "print(data.sum(dim=1))   # SUM OVER ROWS (dimi is the one we're eliminating)\n",
    "print(data.sum(dim=0))   # SUM OVER COLS\n",
    "print(data.std(dim=1))\n",
    "print(data.sum())        # exception: sum() with no dimension is just total sum of the tensor\n",
    "\n",
    "\n",
    "data = torch.tensor([[1,2.2, 9.6], [4, -7.2, 6.3]], dtype = torch.float32)\n",
    "print(data)\n",
    "print(data.mean(dim=0))\n",
    "print(data.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ddab082-03f3-445b-b076-53ec608c6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "# CHECKING SHAPE AND RESHAPING\n",
    "print(ones.shape)\n",
    "\n",
    "# RESHAPING!!!\n",
    "long_line = torch.arange(1, 16)\n",
    "new_line = long_line\n",
    "\n",
    "new_line = new_line.view(5, 3)\n",
    "new_line = new_line.reshape(5, 3)\n",
    "\n",
    "print(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e21d058c-ec7d-4f2d-8e00-95626a68d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 5])\n",
      "[1 0 5]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# CONVERSION\n",
    "import numpy as np\n",
    "\n",
    "np_array = np.array([1,0,5])\n",
    "my_torch_tensor = torch.tensor(np_array)\n",
    "print(my_torch_tensor)\n",
    "\n",
    "my_torch_tensor_to_numpy = my_torch_tensor.numpy()\n",
    "print(my_torch_tensor_to_numpy)\n",
    "\n",
    "# can quickly convert to python value for debugging:\n",
    "print(my_torch_tensor[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b2083-8db9-4ffc-94f8-b3a5a0ce7972",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87869f07-2d75-4172-893f-7bb42f28d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0000,  2.2000,  9.6000],\n",
      "         [ 4.0000, -7.2000,  6.3000]],\n",
      "\n",
      "        [[ 1.0000,  2.0000,  9.0000],\n",
      "         [ 4.0000, -7.0000,  8.3000]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 1.0000,  2.2000,  9.6000],\n",
      "        [ 4.0000, -7.2000,  6.3000]])\n",
      "tensor([[1.0000, 2.2000, 9.6000],\n",
      "        [1.0000, 2.0000, 9.0000]])\n",
      "tensor([[1., 4.],\n",
      "        [1., 4.]])\n",
      "\n",
      "\n",
      "indexing tensors:\n",
      "tensor([[1.0000, 2.2000, 9.6000],\n",
      "        [1.0000, 2.2000, 9.6000],\n",
      "        [1.0000, 2.0000, 9.0000],\n",
      "        [1.0000, 2.0000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "# IT'S JUST A LIST OF LISTS!!!\n",
    "data = torch.tensor([[[1,2.2, 9.6], [4, -7.2, 6.3]], [[1,2, 9], [4, -7, 8.3]]], dtype = torch.float32)\n",
    "print(data)\n",
    "print(data.shape)\n",
    "print(data[0])\n",
    "print(data[:, 0])  # the two outer elements, but the 0th thing in both of those outer elements\n",
    "print(data[:, :, 0]) # just the 0th element in each of the 4 inner\n",
    "\n",
    "print(\"\\n\\nindexing tensors:\")\n",
    "indexing_tensor = torch.tensor([0, 0, 1, 1])\n",
    "j_idx_tensor = torch.tensor([0, 0, 0, 0])\n",
    "print(data[indexing_tensor, j_idx_tensor])    # same as stacking [x[0], x[0], x[1], x[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3efe233-5505-4a07-9a0d-68f06d4c8849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  2.2000,  9.6000],\n",
      "        [ 4.0000, -7.2000,  6.3000]])\n",
      "tensor([1., 4.])\n",
      "tensor([1.0000, 2.2000, 9.6000])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE GETTING A SINGLE ROW OR COL\n",
    "\n",
    "data = torch.Tensor([[1,2.2, 9.6], [4, -7.2, 6.3]])\n",
    "print(data)\n",
    "print(data[:, 0])  # first col\n",
    "print(data[0, :])  # first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f68b5c-d513-4800-b971-a23f838d024f",
   "metadata": {},
   "source": [
    "# AUTOGRAD (MINI BACKPROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8449be8-0c69-48a5-ab3c-b4f1dc1c5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTED DERIVATIVE/GRAD OF SOME FUNCTION: 0.39322386648296376\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as jnp\n",
    "from autograd import grad\n",
    "\n",
    "def tanh(x):\n",
    "    y = jnp.exp(-x)\n",
    "    return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "my_gradient_computing_fn = grad(tanh)\n",
    "print(\"COMPUTED DERIVATIVE/GRAD OF SOME FUNCTION:\", my_gradient_computing_fn(1.0))   # GRADIENT OF tanh(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d455c2-7eb9-419f-863e-12d8d7b02a19",
   "metadata": {},
   "source": [
    "# PYTORCH BACKPROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f18675e9-cf48-4d79-9eb9-59fb415b2f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5721, -0.8316, -0.2502], requires_grad=True)\n",
      "tensor(0.5048, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Basic function:\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5)   # input\n",
    "y = torch.zeros(3)  # expected output\n",
    "\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "print(b)   # [1,1,1,1] vs [[1,1,1]] which you would get from shape (1,3) instead of (3,)\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47bc8541-0daf-4685-a0b8-54ebd645a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1850, 0.0405, 0.1459],\n",
      "        [0.1850, 0.0405, 0.1459],\n",
      "        [0.1850, 0.0405, 0.1459],\n",
      "        [0.1850, 0.0405, 0.1459],\n",
      "        [0.1850, 0.0405, 0.1459]])\n",
      "tensor([0.1850, 0.0405, 0.1459])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()    # just call .backward(), and ANY PYTORCH OBJECT WITH \n",
    "print(w.grad)      # A GRAD ATTRIBUTE WILL GET AUTOMATICALLY COMPUTED\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "42e29104-eec1-4f36-80d3-add099717209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# DISABLE GRADIENT STORAGE ON PYTORCH TENSORS:\n",
    "\n",
    "print(z.requires_grad)\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11573315-fccf-4b46-9445-719e1bd0b007",
   "metadata": {},
   "source": [
    "# PYTORCH NN BACKPROP\n",
    "\n",
    "Pytorch gives us all the modules needed to make neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b77e251b-9e7f-4d26-b436-3e3aa2bf9c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 2])\n",
      "params: [Parameter containing:\n",
      "tensor([[-0.0933, -0.2155, -0.2164,  0.1380],\n",
      "        [ 0.1377,  0.1489, -0.2211, -0.0227]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# LINEAR LAYER\n",
    "import torch.nn as nn\n",
    "\n",
    "my_in = torch.ones(2,3,4)\n",
    "print(my_in.shape)\n",
    "\n",
    "\n",
    "linear = nn.Linear(4, 2, bias=False)   # transforms (N, *, H_in) into (N, *, H_out)\n",
    "linear_output = linear(my_in)\n",
    "print(linear_output.shape)    # col_size 4 -> 2\n",
    "\n",
    "\n",
    "print(\"params:\", list(linear.parameters()))  # w, bias also there by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ad3e86a-b6da-405e-8221-d26371242465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4044, 0.5107],\n",
      "         [0.4044, 0.5107],\n",
      "         [0.4044, 0.5107]],\n",
      "\n",
      "        [[0.4044, 0.5107],\n",
      "         [0.4044, 0.5107],\n",
      "         [0.4044, 0.5107]]], grad_fn=<SigmoidBackward0>)\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# ACTIVATION FUNCTION LAYER - JUST APPLY SOME NONLINEAR FUNCTION TO EACH ELEMENT!!!\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(linear_output)\n",
    "print(output)\n",
    "print(output.shape)    # JUST APPLIES IT ELEMENTWISE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5ec31e8c-64bf-4488-9591-ad2c6da8d728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6770, 0.5666],\n",
      "         [0.6770, 0.5666],\n",
      "         [0.6770, 0.5666]],\n",
      "\n",
      "        [[0.6770, 0.5666],\n",
      "         [0.6770, 0.5666],\n",
      "         [0.6770, 0.5666]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: COMPOSING LAYERS TOGETHER\n",
    "block_of_modules = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "input_0 = torch.ones(2,3,4)\n",
    "output_0 = block_of_modules(input_0)\n",
    "print(output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "003f6135-f491-4239-a619-fd248ec1381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING YOUR OWN MODULE (composed of a bunch of other modules that you can do backprop on)\n",
    "\n",
    "class WillyMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(WillyMLP, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "# also don't have to use nn.Sequential: can also just run it through the blocks yourself:\n",
    "class WillyMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(WillyMLP, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # CAN MANUALLY DEFINE LAYERS AND RUN IT THROUGH\n",
    "        self.linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        relu = self.relu(linear)\n",
    "        linear2 = self.linear2(relu)\n",
    "        output = self.sigmoid(linear2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1088f4b6-98a1-4a8b-92e6-0abf90196b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3667, 0.6227, 0.4619, 0.6093, 0.3967],\n",
      "        [0.4430, 0.7104, 0.3968, 0.6004, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "=================\n",
      "BUNCH OF NAMED PARAMETERS OF OUR NETWORK\n",
      "=================\n",
      "\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-0.2330,  0.1034, -0.2053,  0.2449, -0.0176],\n",
      "        [ 0.3257,  0.3162,  0.1809, -0.2025, -0.1827],\n",
      "        [ 0.4167, -0.4469, -0.2990, -0.3230,  0.3591]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([ 0.2989, -0.4220, -0.1315], requires_grad=True)), ('linear2.weight', Parameter containing:\n",
      "tensor([[-0.4283, -0.4458,  0.3641],\n",
      "        [-0.0695, -0.2243,  0.3731],\n",
      "        [ 0.3718,  0.5350, -0.3076],\n",
      "        [-0.1127, -0.0146, -0.0142],\n",
      "        [-0.3571,  0.1548,  0.5226]], requires_grad=True)), ('linear2.bias', Parameter containing:\n",
      "tensor([-0.4286,  0.5203, -0.2549,  0.4755, -0.3212], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# FORWARD PASS USING OUR CUSTOM NETWORK\n",
    "myi = torch.randn(2, 5)\n",
    "model = WillyMLP(5, 3)\n",
    "print(model(myi))\n",
    "\n",
    "print(\"\\n=================\\nBUNCH OF NAMED PARAMETERS OF OUR NETWORK\\n=================\\n\")\n",
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5304269-dff3-4e5f-b884-66301963e0bc",
   "metadata": {},
   "source": [
    "# TRAINING NEURAL NET CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "074b85ac-b314-4bb3-9163-62eb836dcb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4363e+00,  8.6235e-01,  1.9732e+00,  6.9897e-01, -2.8719e-03],\n",
       "        [ 2.5686e+00,  3.7412e+00,  1.7354e+00,  1.8184e+00,  1.7603e+00],\n",
       "        [ 1.3305e+00,  2.7098e+00,  2.2795e+00,  3.3021e-01,  1.1954e+00],\n",
       "        [ 8.8416e-01,  1.0124e+00,  2.4622e+00,  6.0831e-01,  7.4986e-01],\n",
       "        [ 2.5592e+00,  7.0638e-01,  5.3073e-01,  3.0516e-01,  1.0083e+00],\n",
       "        [ 1.3382e+00,  3.1100e+00,  1.2589e+00, -1.5555e-01,  1.8188e+00],\n",
       "        [ 6.2724e-01,  1.5129e+00,  1.7279e+00,  2.2089e+00,  1.1439e+00],\n",
       "        [ 1.8218e+00,  2.0479e+00,  1.6203e+00,  9.4049e-01,  2.0260e+00],\n",
       "        [ 1.7789e+00,  4.5470e-01,  4.8896e-01,  1.5719e+00, -1.4228e+00],\n",
       "        [ 3.1478e+00,  1.4750e+00,  1.2238e+00,  2.8399e-01,  5.0628e-01]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "y = torch.ones(10, 5)\n",
    "x = y + torch.randn_like(y)  # adding some noise to true y\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d1c92f8-1e0c-42af-bcd9-4140d6bdf408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need your defined model\n",
    "model = WillyMLP(5, 3)\n",
    "\n",
    "# and the OPTIMIZER you are using (adam is fine), also kind of loss youre using\n",
    "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "y_preds = model(x)\n",
    "# print(\"computed loss: \", loss_function(y_preds, y).item())   # COMPUTED LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "46f2558a-d0a5-41c3-8ff5-b9fac0f2279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss is 0.1451420933008194\n",
      "Epoch 1: training loss is 0.06197305768728256\n",
      "Epoch 2: training loss is 0.016157714650034904\n",
      "Epoch 3: training loss is 0.003395551349967718\n",
      "Epoch 4: training loss is 0.0007160273380577564\n",
      "Epoch 5: training loss is 0.00015263627574313432\n",
      "Epoch 6: training loss is 3.269046646892093e-05\n",
      "Epoch 7: training loss is 7.167921467043925e-06\n",
      "Epoch 8: training loss is 1.6430896039310028e-06\n",
      "Epoch 9: training loss is 3.99641436388265e-07\n",
      "Epoch 10: training loss is 1.0407149630964341e-07\n",
      "Epoch 11: training loss is 2.914954144728199e-08\n",
      "Epoch 12: training loss is 8.800101092276691e-09\n",
      "Epoch 13: training loss is 2.863711490164178e-09\n",
      "Epoch 14: training loss is 1.0029017616375313e-09\n",
      "Epoch 15: training loss is 3.776816082101675e-10\n",
      "Epoch 16: training loss is 1.523366710554086e-10\n",
      "Epoch 17: training loss is 6.546344022417827e-11\n",
      "Epoch 18: training loss is 3.0029808500975363e-11\n",
      "Epoch 19: training loss is 1.4462955139071809e-11\n",
      "Epoch 20: training loss is 7.457003764477221e-12\n",
      "Epoch 21: training loss is 4.0250823507559375e-12\n",
      "Epoch 22: training loss is 2.2558311555537225e-12\n",
      "Epoch 23: training loss is 1.3804424183611053e-12\n",
      "Epoch 24: training loss is 8.350297992128219e-13\n"
     ]
    }
   ],
   "source": [
    "# FULL BACKPROP LOOP\n",
    "n_epoch = 25\n",
    "\n",
    "for e in range(n_epoch):\n",
    "    adam.zero_grad()   # set all gradients to 0\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    print(f\"Epoch {e}: training loss is {loss}\")\n",
    "    \n",
    "    loss.backward()  # first compute all the gradients in the model...\n",
    "    adam.step()      # THEN take a step using those computed gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7d52353c-7e53-470b-b9c1-a73f89d18a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LETS SEE THE RESULTS\n",
    "y_pred = model(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a538692-8d1f-4380-b643-d550b24ee2b0",
   "metadata": {},
   "source": [
    "# DATALOADERS AND BATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1286964d-f845-4185-a044-17f256cfaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE OF TRAINING ON GPU WITH A DATALOADER!!!\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neural network for FashionMNIST\n",
    "class FashionMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# IMPORTANT CODE BELOW\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create an instance of the neural network\n",
    "net = FashionMNISTNet()\n",
    "print(net)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the neural network using the FashionMNIST dataset\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        # Move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = running_loss / (i + 1)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc7306-c95d-489d-a91d-97d3ce2b46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOKUP NUM PARAMS\n",
    "\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e867-78f4-4038-8b52-265e37ded6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREEZE OTHER LAYERS: IF YOU JUST WANT TO MODIFY THE VERY LAST LAYER\n",
    "# Now that we have a trained model, if we want to adapt the model to \n",
    "# another dataset with only 5 classes, we can freeze earlier layers and \n",
    "# only train on the last fully-connected layer.\n",
    "\n",
    "# Freeze earlier layers\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = net.fc3.in_features\n",
    "n_classes = 5\n",
    "net.fc3 = nn.Linear(n_inputs, n_classes)\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smol)",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
