{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76141cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-18 23:32:46--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.61MB/s    in 0.4s    \n",
      "\n",
      "2024-12-18 23:32:46 (2.61 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00bd487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0e5a9",
   "metadata": {},
   "source": [
    "# Step 1: Tokenize your text\n",
    "\n",
    "Tokenize essentially means convert every single \"token\"/piece of a word into a __single number__. Here, for simplicity, we will be tokenizing each __character__.\n",
    "\n",
    "GPT-2 uses the byte-pair encoding algorithm. Here we're just going to do a standard character mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e84517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of unique chars:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique chars:  65\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "print(\"List of unique chars: \", unique_chars)\n",
    "print(\"Number of unique chars: \", len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24408e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 46, 47, 57, 1, 47, 57, 1, 39, 1, 57, 58, 56, 47, 52, 45]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = { c:i for i, c in enumerate(unique_chars) }\n",
    "itos = { i:c for i, c in enumerate(unique_chars) }\n",
    "\n",
    "encode = lambda some_str: [stoi[char] for char in some_str]\n",
    "decode = lambda some_str: [itos[char] for char in some_str]\n",
    "\n",
    "\n",
    "example_string = \"This is a string\"\n",
    "encode(example_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d706e85",
   "metadata": {},
   "source": [
    "# Step 1.5: Character -> Token Int -> Token Embedding Vector\n",
    "\n",
    "Step 1: making an embedding lookup table! Each row corresponds to a unique token. The number of that row is equal to the \"number\" of that token (see box above this one for an example: 'T' has token number 32, and thus the vector at row 32 IS 'T''s embedding vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e332a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text))        # turn big list of characters -> big list of token ints\n",
    "\n",
    "train_size = int(0.9 * len(data))        # train/test split: both are just long lists!\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0554a0",
   "metadata": {},
   "source": [
    "## Sidenote: chunking the training data\n",
    "\n",
    "Basically we only ever take in CHUNK_SIZE sequence of chars (taking in all of them at once would be way too hard). CHUNK_SIZE is just the __max length sequence__ we can ever predict on.\n",
    "\n",
    "Here you can see how every CHUNK_SIZE sequence is actually a bunch of training examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19c14b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the training example is tensor([], dtype=torch.int64), the label is 18.\n",
      "When the training example is tensor([18]), the label is 47.\n",
      "When the training example is tensor([18, 47]), the label is 56.\n",
      "When the training example is tensor([18, 47, 56]), the label is 57.\n",
      "When the training example is tensor([18, 47, 56, 57]), the label is 58.\n",
      "When the training example is tensor([18, 47, 56, 57, 58]), the label is 1.\n",
      "When the training example is tensor([18, 47, 56, 57, 58,  1]), the label is 15.\n",
      "When the training example is tensor([18, 47, 56, 57, 58,  1, 15]), the label is 47.\n"
     ]
    }
   ],
   "source": [
    "# Every single chunk is actually a BUNCH of training examples.\n",
    "\n",
    "chunk_size = 8\n",
    "\n",
    "for char_idx in range(chunk_size):\n",
    "    training_example = train_data[:char_idx]\n",
    "    associated_label = train_data[char_idx]\n",
    "    print(f\"When the training example is {training_example}, the label is {associated_label}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b5fc8",
   "metadata": {},
   "source": [
    "# Step 1.6: Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2431543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From train_data, pick tensor([547355, 442444, 711050,  21772]) as starting indices for training sequences.\n",
      "\n",
      "These are the block_size-length sequences starting from each of those indicies:\n",
      "torch.Size([4, 8])\n",
      "tensor([[50, 44,  0, 32, 53,  1, 15, 53],\n",
      "        [43,  1, 58, 46, 43,  1, 51, 59],\n",
      "        [63, 53, 59, 56,  1, 57, 43, 52],\n",
      "        [63, 53, 59, 10,  1, 50, 53, 53]])\n",
      "\n",
      "These are the correct labels associated with each of those example tensors:\n",
      "torch.Size([4, 8])\n",
      "tensor([[44,  0, 32, 53,  1, 15, 53, 59],\n",
      "        [ 1, 58, 46, 43,  1, 51, 59, 56],\n",
      "        [53, 59, 56,  1, 57, 43, 52, 57],\n",
      "        [53, 59, 10,  1, 50, 53, 53, 49]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[50, 44,  0, 32, 53,  1, 15, 53],\n",
       "         [43,  1, 58, 46, 43,  1, 51, 59],\n",
       "         [63, 53, 59, 56,  1, 57, 43, 52],\n",
       "         [63, 53, 59, 10,  1, 50, 53, 53]]),\n",
       " tensor([[44,  0, 32, 53,  1, 15, 53, 59],\n",
       "         [ 1, 58, 46, 43,  1, 51, 59, 56],\n",
       "         [53, 59, 56,  1, 57, 43, 52, 57],\n",
       "         [53, 59, 10,  1, 50, 53, 53, 49]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4     # Batch Size = the NUMBER of sequences we forward pass, backward pass, and step with every epoch.\n",
    "block_size = 8     # Block_Size = maximum context length for a prediction.\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # for this particular batch, we want to get 4 sequences each of sequence length 32\n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))     # get 4 indexes into 'data': the indexes can only be from 0 to len(data) - block_size\n",
    "                                                                                            # this will be a 1D tensor of size (batch_size,), i.e. tensor([953063, 497175, 633405, 627354])\n",
    "    print(f\"From {split}_data, pick {random_starting_idx_of_batch} as starting indices for training sequences.\")\n",
    "    \n",
    "    # now that we have some starting indices, pick out the 32-length sequence from each of them\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "    print(\"\\nThese are the block_size-length sequences starting from each of those indicies:\")\n",
    "    print(training_sequences_tensor.shape)\n",
    "    print(training_sequences_tensor)\n",
    "    \n",
    "    \n",
    "    # now we'll get a tensor, but with all the relevant labels. Remember we're using the trick above to get more examples.\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    print(\"\\nThese are the correct labels associated with each of those example tensors:\")\n",
    "    print(labels_tensor.shape)\n",
    "    print(labels_tensor)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "    \n",
    "    \n",
    "get_batch('train')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbfaec",
   "metadata": {},
   "source": [
    "# Step 2: Forward Pass\n",
    "\n",
    "Fantastic. Now we've got the training input data and labels in a really nice, batched format. Now we'll make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e639b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From train_data, pick tensor([ 76049, 234249, 934904, 560986]) as starting indices for training sequences.\n",
      "\n",
      "These are the block_size-length sequences starting from each of those indicies:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "These are the correct labels associated with each of those example tensors:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "Batched input is originally torch.Size([4, 8]).\n",
      "After doing embedding lookup, it is torch.Size([4, 8, 65]), since we replace each int token with a 65-sized vector.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    # model has internal embedding vector lookup table based on token\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    # forward pass: foreach INT_TOKEN in input_batched, turn that into the correct embedding vector,\n",
    "    # and then replace that INT_TOKEN with the relevant embedding vector.\n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        logits = self.token_embedding_table(input_batched) \n",
    "        print(f\"\\nBatched input is originally {input_batched.shape}.\")\n",
    "        print(f\"After doing embedding lookup, it is {logits.shape}, since we replace each int token with a {len(unique_chars)}-sized vector.\")\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "out = m.forward(xb, yb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fc3c8",
   "metadata": {},
   "source": [
    "# Step 2.5: Formulate with Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efdbab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- GENERATING SEQUENCE --------\n",
      "\n",
      "sV\n",
      "vL\n",
      "ja,FsLY,wxEuS'pao3jOssyBA$zFqYTkeMk x-gQ.FzLg!iKI.egzDnyA TsTbvdgX!KpGIeJyjv,SrFF&SDt!:hwWSl.W\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # for this particular batch, we want to get 4 sequences each of sequence length 32\n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))     # get 4 indexes into 'data': the indexes can only be from 0 to len(data) - block_size\n",
    "                                                                                            # this will be a 1D tensor of size (batch_size,), i.e. tensor([953063, 497175, 633405, 627354])\n",
    "#     print(f\"From {split}_data, pick {random_starting_idx_of_batch} as starting indices for training sequences.\")\n",
    "    \n",
    "    # now that we have some starting indices, pick out the 32-length sequence from each of them\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "#     print(\"\\nThese are the block_size-length sequences starting from each of those indicies:\")\n",
    "#     print(training_sequences_tensor.shape)\n",
    "#     print(training_sequences_tensor)\n",
    "    \n",
    "    \n",
    "    # now we'll get a tensor, but with all the relevant labels. Remember we're using the trick above to get more examples.\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "#     print(\"\\nThese are the correct labels associated with each of those example tensors:\")\n",
    "#     print(labels_tensor.shape)\n",
    "#     print(labels_tensor)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    # model has internal embedding vector lookup table based on token\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    # forward pass: foreach INT_TOKEN in input_batched, turn that into the correct embedding vector,\n",
    "    # and then replace that INT_TOKEN with the relevant embedding vector.\n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        logits = self.token_embedding_table(input_batched)           # replace all INT_TOKENS with vector embeddings\n",
    "        \n",
    "        \n",
    "        if target_batched is None:\n",
    "            loss = None                                              # If no target is provided,\n",
    "        else:\n",
    "            batch_size, sequence_length, embedding_dim = logits.shape    \n",
    "            logits = logits.view(batch_size * sequence_length, embedding_dim)   # stack ALL embeddings in a SINGLE LIST\n",
    "\n",
    "            targets = target_batched.view(batch_size * sequence_length)    # do same for target INT_TOKENS: there are 4 sequences per batch, with each sequence having 8 characters\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)        # provide a tensor of embeddings, and a tensor of targets, and it will find the loss\n",
    "\n",
    "        ### --------------------------------------\n",
    "        \n",
    "#         print(\"\\nUnstackify the logits tensor:\")\n",
    "#         print(logits.shape)\n",
    "#         print(logits)\n",
    "        \n",
    "#         if target_batched is not None:\n",
    "#             print(\"\\nUnstackify the targets tensor:\")\n",
    "#             print(targets.shape)\n",
    "#             print(targets)\n",
    "#             print(f\"\\nThe logit and associated target can now be matched 1-to-1. The cross-entropy loss is {loss:.4f}.\")\n",
    "        ### --------------------------------------\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, starting_sequence_tensor, max_new_tokens):\n",
    "        \n",
    "        \n",
    "        print(\"\\n-------- GENERATING SEQUENCE --------\")\n",
    "        \n",
    "        # repeat for the number of new tokens you want...\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(starting_sequence_tensor, None)        # forward pass our input sequence: basically just convering INT_TOKENS to associated embedding vector\n",
    "#             print(logits.shape)                                                # batch_size  x  single_batch_sequence_length  x  embedding_dimension\n",
    "            logits = logits[:, -1, :]                                    # from our logits tensor (list of embedding vectors), we only care about the very LAST one in the timestep\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)     # convert logits (of vocab-size) to probability distribution             \n",
    "            generated_int_token = torch.multinomial(probs, num_samples=1)        # sample from that probability distribution\n",
    "            starting_sequence_tensor = torch.cat((starting_sequence_tensor, generated_int_token), dim=1)        # append that new token\n",
    "#             print(f\"starting_sequence_tensor shape: {starting_sequence_tensor.shape}\")\n",
    "            \n",
    "        return starting_sequence_tensor\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "# out, loss = m.forward(xb, yb)\n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)\n",
    "\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f959201",
   "metadata": {},
   "source": [
    "# Step 3: Training\n",
    "\n",
    "Great, now we can do a forward pass that does input_int_token_sequence -> embedding_vector_sequence/logits. We can also convert those logits to a probability distribution via a softmax, and then sample from that distribution to get a new token, which we can append back onto the original input_int_token_sequence and continue onwards.\n",
    "\n",
    "Now let's train our model parameters (which is just m.parameters() - remember that m is a nn.Module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e28a39b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is: 2.5045759677886963\n",
      "loss at epoch 2000 is: 2.4128713607788086\n",
      "loss at epoch 4000 is: 2.4660370349884033\n",
      "loss at epoch 6000 is: 2.4909756183624268\n",
      "loss at epoch 8000 is: 2.3688089847564697\n",
      "\n",
      "-------- GENERATING SEQUENCE --------\n",
      "\n",
      "Yourete fay MI RIOPUCap t waug whassely sy e msbe shes, d th, h youre w ag mur ore irt\n",
      "Ano and t wis\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batched, label_batched = get_batch('train')\n",
    "    \n",
    "    logits, loss = m.forward(input_batched, label_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)                     # RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'loss at epoch {epoch} is: {loss.item()}')\n",
    "\n",
    "    \n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))\n",
    "\n",
    "\n",
    "# pretty good! The only issue is that we're only using the last logit-embedding-vector to predict the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacfba8",
   "metadata": {},
   "source": [
    "# SECTION II: Self-Attention\n",
    "\n",
    "How do we get tokens to talk to each other?\n",
    "\n",
    "- Don't want future tokens to communicate to past tokens. You're trying to PREDICT the future given the past! It's not like given the past AND the future, predict the intermediate.\n",
    "\n",
    "- Naive Solution 1: if you're token 5, take tokens 1-4, average them up, and the resulting vector is a \"context\" summarizing token 5 in the CONTEXT of what came before.\n",
    "    - Issue: Averaging/sum is extremely lossy: you've lost spatial/positional information.\n",
    "    \n",
    "    \n",
    "Let's do this naive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bd9eb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size, seq_length, embedding_size = 4, 8, 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_length, embedding_size)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c575e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication 1: want x[b, t] = mean of x[b, i] where i <= t\n",
    "mean_embeddings = torch.zeros((batch_size, seq_length, embedding_size))\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    for token_idx in range(seq_length):\n",
    "        prev_embeddings = x[batch, : token_idx+1]\n",
    "        mean_embeddings[batch, token_idx] = torch.mean(prev_embeddings, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aad02a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72be49e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice: each subsequence timestep \"incorporates\" information from all the previous timesteps! We just take an avg!\n",
    "mean_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fefd081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b = \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = \n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "\n",
      " Notice how multiplying by a lower-triangular ones matrix allows you to incrementally add each vector of b!\n"
     ]
    }
   ],
   "source": [
    "# MATHEMATICAL TRICK TO MAKING THE ABOVE CODE EFFICIENT VIA MATRIX MULTIPLICATIONS\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "a = torch.tril(a)    # convert a to lower-triangular\n",
    "\n",
    "\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"c = \")\n",
    "print(c)\n",
    "\n",
    "print(\"\\n Notice how multiplying by a lower-triangular ones matrix allows you to incrementally add each vector of b!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d96c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b = \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = \n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "\n",
      "Notice how a's rows sum to 1! This lets you compute the incremental-acerage as described!\n",
      "Think through a @ b, and notice how you're incrementally averaging all the previous rows!\n"
     ]
    }
   ],
   "source": [
    "# You can thus easily compute the average by making the row-elems of a to sum to 1!\n",
    "# MATHEMATICAL TRICK TO MAKING THE ABOVE CODE EFFICIENT VIA MATRIX MULTIPLICATIONS\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "a = torch.tril(a)    # convert a to lower-triangular\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"c = \")\n",
    "print(c)\n",
    "\n",
    "print(\"\\nNotice how a's rows sum to 1! This lets you compute the incremental-acerage as described!\\nThink through a @ b, and notice how you're incrementally averaging all the previous rows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c160022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embeddings =\n",
      "tensor([[ 1.2946,  0.2227],\n",
      "        [-1.2924,  0.1689],\n",
      "        [-0.8326, -0.8129],\n",
      "        [ 0.9700, -0.6758]])\n",
      "averaging_matrix =\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "averaged_input_embeddings =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2946e+00,  2.2267e-01],\n",
       "        [ 1.1061e-03,  1.9577e-01],\n",
       "        [-2.7680e-01, -1.4047e-01],\n",
       "        [ 3.4909e-02, -2.7429e-01]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do it on our larger toy example\n",
    "# batch_size, seq_length, embedding_size = 4, 8, 2\n",
    "B, T, C = 4, 4, 2\n",
    "\n",
    "input_embeddings = torch.randn(B, T, C)   # within a single batch, our goal is to get the \"incremental averaging\" of all the previous vectors\n",
    "\n",
    "\n",
    "averaging_matrix = torch.ones(T, T)       # the averaging matrix is seq_length x seq_length\n",
    "averaging_matrix = torch.tril(averaging_matrix)\n",
    "averaging_matrix = averaging_matrix / torch.sum(averaging_matrix, 1, keepdim=True)\n",
    "\n",
    "\n",
    "print(\"input_embeddings =\")\n",
    "print(input_embeddings[0])\n",
    "\n",
    "print(\"averaging_matrix =\")\n",
    "print(averaging_matrix)\n",
    "\n",
    "averaged_input_embeddings = averaging_matrix @ input_embeddings   # little strange, but this applies the matrix to each batch\n",
    "                                                                  # (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) -> (B, T, C)    :     averaging matrix is broadcast along batch dim\n",
    "\n",
    "print(\"averaged_input_embeddings =\")\n",
    "averaged_input_embeddings[0]\n",
    "\n",
    "\n",
    "# AVERAGING MATRIX IS A WEIGHT MATRIX! TOKEN GETS INFO ONLY FROM TOKENS PREVIOUS TO IT \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18d52c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small trick: Note that for softmax, we actually set the upper triangle of averaging_matrix to '-inf': so that AFTER\n",
    "# the softmax those entries get turned to 0.\n",
    "\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "\n",
    "weights_matrix = torch.zeros((T, T))\n",
    "weights_matrix = weights_matrix.masked_fill(tril == 0, float('-inf'))   # tokens from the future cna't communicate with the past\n",
    "weights_matrix = F.softmax(weights_matrix, dim=-1)\n",
    "\n",
    "weights_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c759b8",
   "metadata": {},
   "source": [
    "__KEY TAKEAWAY__: Given some list of embeddings, you can do _weighted aggregations_ of your *past* sequence embeddings via _matrix multiplication_ with a lower-triangular matrix, where the elements in the lower-triangular part tell you how much each element fuses into the current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "959b45a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n",
      "Attention weights for a SINGLE batch: notice how it's not equally weighted anymore!\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "In this specific example, looking at the last row, the 8th token sees the 7th and 4th token (0.2423 and 0.2297 respectively) as highly relevant w.r.t. their query and key vectors being aligned.\n",
      "\n",
      "Shape of the output:\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# SELF-ATTENTION FULL CODE\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32        # batch, time, channels/embedding size\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "\n",
    "# this code does a rolling average of previous embeddings via a low-triangular matrix\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ x\n",
    "\n",
    "print(\"OG weights:\\n\", weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ABSOLUTELY CRITICAL:\n",
    "# Different tokens find other tokens more or less interesting! Different past tokens should have different weights, currently OG weights just weights all past tokens equally!\n",
    "\n",
    "# Soln: for each embedding in my sequence, emit a query (what I'm looking for) and key (what do I contain) vectors.\n",
    "# The dot product btwn key and query IS my weight! If they're more aligned, I learn MORE from that specific token!\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)        # (B, T, 16)\n",
    "q = query(x)      # (B, T, 16)\n",
    "v = value(x)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1)    # (B, T, 16) @ (B, 16, T)    --->    (B, T, T)     :     This is our better weight matrix!\n",
    "# weights = torch.zeros((T, T))      # no longer zeros...\n",
    "\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))        # we make the future weights all -inf so they become 0 during the softmax\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ v\n",
    "\n",
    "print(\"\\nAttention weights for a SINGLE batch: notice how it's not equally weighted anymore!\\n\", weights[0])\n",
    "\n",
    "print(\"\\nIn this specific example, looking at the last row, the 8th token sees the 7th and 4th token (0.2423 and 0.2297 respectively) as highly relevant w.r.t. their query and key vectors being aligned.\")\n",
    "\n",
    "\n",
    "print(\"\\nShape of the output:\")\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bf1eb",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "    - Graph of nodes that get information from itself + all nodes that connect to it. Communication in a directed graph!\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "    - Vector gets a thing added to it that just denotes its specific position\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "    - BATCHES ALL INDEPENDENT\n",
    "\n",
    "\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "    - the masked_fill tril lines prevent future from affecting past: in a sentiment analysis app you WOULD want the future to affect the past: so delete those lines with the tril.\n",
    "    - \"decoder\" in the sense that this triangular masking lets us do autoregressive language modeling\n",
    "    - attention is just ARBITRARY COMMUNICATION ACROSS VECTORS/NODES\n",
    "\n",
    "\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "    - \"cross-attention\" means the keys and values come from ELSEWHERE\n",
    "\n",
    "\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n",
    "    - basically SOFTMAX on LARGE NUMBERS will put a LOT more weight on the largest node: so we just divide by this term to prevent this bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd202e6",
   "metadata": {},
   "source": [
    "# SECTION III: Let's add this self-attention block to the language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04c8b523",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32]' is invalid for input of size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m    131\u001b[0m     input_batched, label_batched \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)                     \u001b[38;5;66;03m# RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[111], line 95\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, input_batched, target_batched)\u001b[0m\n\u001b[1;32m     92\u001b[0m     batch_size, sequence_length, embedding_dim \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape    \n\u001b[1;32m     93\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m sequence_length, embedding_dim)\n\u001b[0;32m---> 95\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_batched\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     96\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32]' is invalid for input of size 256"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "\n",
    "\n",
    "embedding_dim = 32\n",
    "max_context_length = 8\n",
    "block_size = max_context_length\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "                \n",
    "        # linear layers applied to all nodes\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias = False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias = False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias = False) \n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))   # tril just needs t go to special buffer\n",
    "    \n",
    "    def forward(self, input_sequence_batched):\n",
    "        B, T, C = input_sequence_batched.shape   # remember: B is batch size, T is sequence length, C is the channels/num dimensions per embedding\n",
    "        \n",
    "        keys_batched = self.key(x)  # B, T, C   -> broadcasted along batch channel\n",
    "        queries_batched = self.query(x)\n",
    "        \n",
    "        # GET THE BIG MATRIX OF QK^T DOT PRODUCTS:\n",
    "        wei = queries_batched @ keys_batched.transpose(-2, -1) * (C ** -5)   # getsqrt(d_k) term in the paper\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))        # mask that big Q-K matrix\n",
    "        wei = F.softmax(wei, dim = 1)                                       # then softmax each column\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v                                                       # multiply that by the value matrix, and those are our information-soaked-up logits!\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)        \n",
    "        self.position_embedding_table = nn.Embedding(max_context_length, embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.sa_head = Head(embedding_dim)                     # convert vector_embedding -> attentionified_vector_embedding\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)    # convert vector_embedding -> vocab_size (REAL logits)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        sequence_token_embeddings = self.token_embedding_table(input_batched)\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T))   # 0....T-1 for a (T, embedding_dim) matrix\n",
    "        \n",
    "        x = sequence_token_embeddings + positional_embedding                    # this is how you add positional infomration into the sequence\n",
    "        \n",
    "        x = self.sa_head(x)         # Apply attention block\n",
    "        logits = self.lm_head(x)    # Decode back to vocab-sized logits (right before softmaxxing and sampling a word)\n",
    "        \n",
    "        \n",
    "        if target_batched is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, embedding_dim = logits.shape    \n",
    "            logits = logits.view(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "            targets = target_batched.view(batch_size * sequence_length) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, starting_sequence_tensor, max_new_tokens):\n",
    "        \n",
    "        \n",
    "        print(\"\\n-------- GENERATING SEQUENCE --------\")\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            starting_sequence_tensor = starting_sequence_tensor[:, -block_size:]   # crop \"input context\" to just the last block_size tokens... next token depends on last ~8 tokens (otherwise the input dimensions are off)\n",
    "            \n",
    "            logits, loss = self.forward(starting_sequence_tensor, None)        # forward pass our input sequence: basically just convering INT_TOKENS to associated embedding vector\n",
    "            logits = logits[:, -1, :]                                          # from our logits tensor (list of embedding vectors), we only care about the very LAST one in the timestep\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                                  # convert logits (of vocab-size) to probability distribution             \n",
    "            generated_int_token = torch.multinomial(probs, num_samples=1)      # sample from that probability distribution\n",
    "            starting_sequence_tensor = torch.cat((starting_sequence_tensor, generated_int_token), dim=1)        # append that new token\n",
    "            \n",
    "        return starting_sequence_tensor\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batched, label_batched = get_batch('train')\n",
    "    \n",
    "    logits, loss = m.forward(input_batched, label_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)                     # RESET THE ACCUMULATED GRADIENTS WITH EVERY NEW BATCH!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'loss at epoch {epoch} is: {loss.item()}')\n",
    "\n",
    "    \n",
    "generated_token_sequence = m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)\n",
    "print(''.join(decode(generated_token_sequence[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240e6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d28a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199d950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c6e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ccdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smol)",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
