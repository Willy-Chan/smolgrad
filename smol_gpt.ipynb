{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76141cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-18 23:32:46--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.61MB/s    in 0.4s    \n",
      "\n",
      "2024-12-18 23:32:46 (2.61 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00bd487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0e5a9",
   "metadata": {},
   "source": [
    "# Step 1: Tokenize your text\n",
    "\n",
    "Tokenize essentially means convert every single \"token\"/piece of a word into a __single number__. Here, for simplicity, we will be tokenizing each __character__.\n",
    "\n",
    "GPT-2 uses the byte-pair encoding algorithm. Here we're just going to do a standard character mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e84517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of unique chars:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique chars:  65\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "print(\"List of unique chars: \", unique_chars)\n",
    "print(\"Number of unique chars: \", len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24408e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 46, 47, 57, 1, 47, 57, 1, 39, 1, 57, 58, 56, 47, 52, 45]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = { c:i for i, c in enumerate(unique_chars) }\n",
    "itos = { i:c for i, c in enumerate(unique_chars) }\n",
    "\n",
    "encode = lambda some_str: [stoi[char] for char in some_str]\n",
    "decode = lambda some_str: [itos[char] for char in some_str]\n",
    "\n",
    "\n",
    "example_string = \"This is a string\"\n",
    "encode(example_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d706e85",
   "metadata": {},
   "source": [
    "# Step 1.5: Character -> Token Int -> Token Embedding Vector\n",
    "\n",
    "Step 1: making an embedding lookup table! Each row corresponds to a unique token. The number of that row is equal to the \"number\" of that token (see box above this one for an example: 'T' has token number 32, and thus the vector at row 32 IS 'T''s embedding vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e332a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text))        # turn big list of characters -> big list of token ints\n",
    "\n",
    "train_size = int(0.9 * len(data))        # train/test split: both are just long lists!\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0554a0",
   "metadata": {},
   "source": [
    "## Sidenote: chunking the training data\n",
    "\n",
    "Basically we only ever take in CHUNK_SIZE sequence of chars (taking in all of them at once would be way too hard). CHUNK_SIZE is just the __max length sequence__ we can ever predict on.\n",
    "\n",
    "Here you can see how every CHUNK_SIZE sequence is actually a bunch of training examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f19c14b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the training example is tensor([], dtype=torch.int64), the label is 18.\n",
      "When the training example is tensor([18]), the label is 47.\n",
      "When the training example is tensor([18, 47]), the label is 56.\n",
      "When the training example is tensor([18, 47, 56]), the label is 57.\n",
      "When the training example is tensor([18, 47, 56, 57]), the label is 58.\n",
      "When the training example is tensor([18, 47, 56, 57, 58]), the label is 1.\n",
      "When the training example is tensor([18, 47, 56, 57, 58,  1]), the label is 15.\n",
      "When the training example is tensor([18, 47, 56, 57, 58,  1, 15]), the label is 47.\n"
     ]
    }
   ],
   "source": [
    "# Every single chunk is actually a BUNCH of training examples.\n",
    "\n",
    "chunk_size = 8\n",
    "\n",
    "for char_idx in range(chunk_size):\n",
    "    training_example = training_data[:char_idx]\n",
    "    associated_label = training_data[char_idx]\n",
    "    print(f\"When the training example is {training_example}, the label is {associated_label}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b5fc8",
   "metadata": {},
   "source": [
    "# Step 1.6: Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2431543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From train_data, pick tensor([971401, 579495, 193625, 348340]) as starting indices for training sequences.\n",
      "\n",
      "These are the block_size-length sequences starting from each of those indicies:\n",
      "torch.Size([4, 8])\n",
      "tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46]])\n",
      "\n",
      "These are the correct labels associated with each of those example tensors:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
       "         [60, 43, 42,  8,  0, 25, 63,  1],\n",
       "         [56, 42,  5, 57,  1, 57, 39, 49],\n",
       "         [43, 57, 58, 63,  6,  1, 58, 46]]),\n",
       " tensor([[43, 60, 43, 52,  1, 63, 43, 39],\n",
       "         [43, 42,  8,  0, 25, 63,  1, 45],\n",
       "         [42,  5, 57,  1, 57, 39, 49, 43],\n",
       "         [57, 58, 63,  6,  1, 58, 46, 47]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4     # Batch Size = the NUMBER of sequences we forward pass, backward pass, and step with every epoch.\n",
    "block_size = 8     # Block_Size = maximum context length for a prediction.\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # for this particular batch, we want to get 4 sequences each of sequence length 32\n",
    "    random_starting_idx_of_batch = torch.randint(len(data) - block_size, (batch_size,))     # get 4 indexes into 'data': the indexes can only be from 0 to len(data) - block_size\n",
    "                                                                                            # this will be a 1D tensor of size (batch_size,), i.e. tensor([953063, 497175, 633405, 627354])\n",
    "    print(f\"From {split}_data, pick {random_starting_idx_of_batch} as starting indices for training sequences.\")\n",
    "    \n",
    "    # now that we have some starting indices, pick out the 32-length sequence from each of them\n",
    "    training_sequences = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        training_sequences.append(data[starting_index : starting_index + block_size])\n",
    "        \n",
    "    training_sequences_tensor = torch.stack(training_sequences)\n",
    "    print(\"\\nThese are the block_size-length sequences starting from each of those indicies:\")\n",
    "    print(training_sequences_tensor.shape)\n",
    "    print(training_sequences_tensor)\n",
    "    \n",
    "    \n",
    "    # now we'll get a tensor, but with all the relevant labels. Remember we're using the trick above to get more examples.\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        starting_index = random_starting_idx_of_batch[i]\n",
    "        labels.append(data[starting_index + 1 : starting_index + block_size + 1])\n",
    "        \n",
    "    labels_tensor = torch.stack(labels)\n",
    "    print(\"\\nThese are the correct labels associated with each of those example tensors:\")\n",
    "    print(labels_tensor.shape)\n",
    "    print(labels_tensor)\n",
    "    \n",
    "    return training_sequences_tensor, labels_tensor\n",
    "    \n",
    "    \n",
    "get_batch('train')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbfaec",
   "metadata": {},
   "source": [
    "# Step 2: Forward Pass\n",
    "\n",
    "Fantastic. Now we've got the training input data and labels in a really nice, batched format. Now we'll make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e639b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From train_data, pick tensor([ 76049, 234249, 934904, 560986]) as starting indices for training sequences.\n",
      "\n",
      "These are the block_size-length sequences starting from each of those indicies:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "These are the correct labels associated with each of those example tensors:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "Batched input is originally torch.Size([4, 8]).\n",
      "After doing embedding lookup, it is torch.Size([4, 8, 65]), since we replace each int token with a 65-sized vector.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    # model has internal embedding vector lookup table based on token\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    # forward pass: foreach INT_TOKEN in input_batched, turn that into the correct embedding vector,\n",
    "    # and then replace that INT_TOKEN with the relevant embedding vector.\n",
    "    def forward(self, input_batched, target_batched):\n",
    "        \n",
    "        logits = self.token_embedding_table(input_batched) \n",
    "        print(f\"\\nBatched input is originally {input_batched.shape}.\")\n",
    "        print(f\"After doing embedding lookup, it is {logits.shape}, since we replace each int token with a {len(unique_chars)}-sized vector.\")\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "m = BigramLanguageModel(vocab_size = len(unique_chars))\n",
    "out = m.forward(xb, yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e58c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbab15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smol)",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
